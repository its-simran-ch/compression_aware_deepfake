% ============================================================
% IEEE-Style Paper Draft Skeleton
% Compression-Aware Video Deepfake Detection Framework
% Using Spatial and Frequency-Domain Features
% ============================================================
% Compile with: pdflatex draft_skeleton.tex
%               bibtex draft_skeleton
%               pdflatex draft_skeleton.tex (x2)
% ============================================================

\documentclass[conference]{IEEEtran}

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{xcolor}

% Placeholder command for values to be filled in
\newcommand{\placeholder}[1]{\textcolor{red}{\textbf{#1}}}

\begin{document}

\title{A Compression-Aware Video Deepfake Detection Framework Using Spatial and Frequency-Domain Features}

\author{
    \IEEEauthorblockN{Simran Chaudhary}
    \IEEEauthorblockA{
        Department of Artificial Intelligence \\
        \placeholder{University Name} \\
        \placeholder{City, Country} \\
        \placeholder{email@university.edu}
    }
}

\maketitle

% ============================================================
% ABSTRACT
% ============================================================
\begin{abstract}
Video deepfake detection has emerged as a critical challenge in multimedia forensics, yet most existing detectors suffer significant performance degradation when confronted with compressed video — the dominant format for online distribution. In this paper, we propose a compression-aware hybrid framework that combines spatial features extracted via EfficientNet-B0 with frequency-domain features obtained through Discrete Wavelet Transform (DWT) processing and a lightweight CNN. These complementary feature representations are fused through concatenation and a multi-layer perceptron for frame-level binary classification. We conduct a systematic evaluation on the FaceForensics++ dataset across three compression levels (c0, c23, and c40) and perform cross-dataset testing on Celeb-DF v2. Our experimental results demonstrate that the proposed hybrid approach achieves an AUC of \placeholder{AUC\_c40\_hybrid} on heavily compressed (c40) video, compared to \placeholder{AUC\_c40\_spatial} for the spatial-only baseline — a \placeholder{X\%} relative improvement. Ablation studies confirm that the frequency branch provides the most significant benefit under heavy compression, where spatial artifacts are masked by quantization noise. The framework is designed for reproducibility and practical deployment, with all code publicly available.
\end{abstract}

\begin{IEEEkeywords}
deepfake detection, video forensics, compression robustness, EfficientNet, discrete wavelet transform, frequency analysis
\end{IEEEkeywords}

% ============================================================
% I. INTRODUCTION
% ============================================================
\section{Introduction}

The rapid advancement of deep learning-based face manipulation technologies, commonly referred to as ``deepfakes,'' poses significant threats to information integrity, personal privacy, and public trust~\cite{rossler2019faceforensics}. While numerous detection methods have been proposed, a critical yet often overlooked challenge is the impact of video compression on detector performance.

In practical scenarios, videos undergo compression during recording, storage, and distribution through platforms such as YouTube, WhatsApp, and social media. The most widely used codec, H.264, introduces quantization artifacts that can mask the subtle manipulation traces that detectors rely on. Existing benchmark evaluations often focus on raw or lightly compressed data, providing an overly optimistic view of real-world detection capability.

To address this gap, we propose a \textit{compression-aware} hybrid detection framework that leverages two complementary feature streams:
\begin{itemize}
    \item \textbf{Spatial branch}: An EfficientNet-B0 backbone extracts high-level semantic features from face crops, capturing visible manipulation artifacts such as blending boundaries and texture inconsistencies.
    \item \textbf{Frequency branch}: A 2D Discrete Wavelet Transform (DWT) decomposes face images into frequency subbands, and a lightweight CNN extracts features that capture spectral-domain manipulation artifacts, which behave differently under compression.
\end{itemize}

Our key contributions are:
\begin{enumerate}
    \item A hybrid spatial-frequency architecture that demonstrates improved robustness under varying compression levels.
    \item A systematic evaluation protocol across three H.264 compression quality factors (c0, c23, c40) on FaceForensics++.
    \item Cross-dataset generalization analysis using Celeb-DF v2.
    \item An open-source implementation with an interactive demonstration system.
\end{enumerate}

% ============================================================
% II. RELATED WORK
% ============================================================
\section{Related Work}

\subsection{CNN-Based Deepfake Detection}
Modern deepfake detectors predominantly rely on convolutional neural networks trained on face crops. Rössler et al.~\cite{rossler2019faceforensics} established the FaceForensics++ benchmark and demonstrated strong performance using XceptionNet. Subsequent works have explored various architectures including EfficientNet~\cite{tan2019efficientnet} variants, achieving high accuracy on standard benchmarks.

\subsection{Frequency-Domain Analysis}
Several works have demonstrated that frequency-domain analysis can reveal GAN-generated artifacts invisible in the spatial domain. Durall et al.~\cite{durall2020watch} showed that GANs fail to reproduce natural spectral distributions. Frank et al.~\cite{frank2020leveraging} proposed using DCT coefficients for detection. Qian et al.~\cite{qian2020thinking} introduced F3-Net, which mines frequency-aware clues using DCT. Our approach uses DWT instead of DCT, providing both spatial and frequency localization.

\subsection{Compression Effects on Detection}
The impact of compression on detection performance has been observed but not systematically studied with hybrid approaches. Standard compression (H.264 with quality factors like CRF 23 and 40) removes high-frequency information — precisely where many manipulation artifacts reside. Our work directly addresses this by evaluating across compression levels and incorporating frequency-domain features that capture compression-specific patterns.

% ============================================================
% III. METHODOLOGY
% ============================================================
\section{Methodology}

\subsection{Overview}
Our framework processes input videos through a pipeline: frame sampling $\rightarrow$ face detection $\rightarrow$ dual-branch feature extraction $\rightarrow$ fusion $\rightarrow$ classification. Fig.~\ref{fig:architecture} illustrates the overall architecture.

% TODO: Add architecture diagram
% \begin{figure}[t]
% \centering
% \includegraphics[width=\columnwidth]{figures/architecture.png}
% \caption{Overview of the proposed compression-aware hybrid deepfake detection framework.}
% \label{fig:architecture}
% \end{figure}

\subsection{Preprocessing}
Given an input video, we sample frames at 5 fps (maximum 100 frames). For each frame, we detect the largest face using MTCNN~\cite{zhang2016mtcnn} with a margin of 40 pixels and resize to $224 \times 224$ pixels.

\subsection{Spatial Branch}
We employ EfficientNet-B0~\cite{tan2019efficientnet} pretrained on ImageNet as our spatial feature extractor. The classification head is removed, yielding a 1280-dimensional feature vector per face crop. The compound scaling of EfficientNet provides an effective balance between accuracy and computational efficiency suitable for our MSc-level scope.

\subsection{Frequency Branch}
The frequency branch applies a 2D Discrete Wavelet Transform using the Haar (db1) wavelet to grayscale face crops:
\begin{equation}
(c_A, c_H, c_V, c_D) = \text{DWT}_{2D}(I_{gray})
\end{equation}
where $c_A$ is the approximation coefficient and $c_H, c_V, c_D$ are horizontal, vertical, and diagonal detail coefficients respectively. These four subbands are stacked as a 4-channel tensor of size $4 \times 112 \times 112$ and processed through a lightweight CNN (4 convolutional blocks with batch normalization and max pooling) to produce a 128-dimensional frequency feature vector.

\subsection{Feature Fusion and Classification}
The spatial (1280-D) and frequency (128-D) features are concatenated to form a 1408-dimensional vector, which is passed through a two-layer MLP:
\begin{equation}
\hat{y} = \sigma(W_2 \cdot \text{ReLU}(W_1 \cdot [f_s \| f_f] + b_1) + b_2)
\end{equation}
where $f_s$ and $f_f$ denote spatial and frequency features, $\|$ denotes concatenation, and $\sigma$ is the sigmoid function. Dropout ($p=0.3$) is applied after each hidden layer for regularization.

\subsection{Video-Level Aggregation}
Frame-level probabilities are averaged to produce a video-level prediction:
\begin{equation}
P_{video} = \frac{1}{N} \sum_{i=1}^{N} P(fake | frame_i)
\end{equation}
A video is classified as FAKE if $P_{video} \geq 0.55$, REAL if $P_{video} \leq 0.45$, and UNCERTAIN otherwise.

% ============================================================
% IV. EXPERIMENTS
% ============================================================
\section{Experiments}

\subsection{Datasets}
\textbf{FaceForensics++ (FF++)}~\cite{rossler2019faceforensics}: We use the original sequences and two manipulation methods (Deepfakes, FaceSwap) across three compression levels: c0 (raw), c23 (light H.264, CRF 23), and c40 (heavy, CRF 40). The official split of 720/140/140 (train/val/test) videos is used.

\textbf{Celeb-DF v2}~\cite{li2020celebdf}: A challenging dataset with 590 real and 5,639 synthesized videos, used exclusively for cross-dataset evaluation.

\subsection{Implementation Details}
All experiments use PyTorch 2.0+ on NVIDIA T4 GPUs (Google Colab). Training employs AdamW optimizer ($lr=10^{-4}$, weight decay $10^{-5}$) with cosine annealing schedule over 15 epochs. Batch size is 16. Data augmentation includes random horizontal flipping, color jitter, and small rotations ($\pm 5°$).

\subsection{Evaluation Metrics}
We report frame-level Accuracy, Precision, Recall, F1-Score, and Area Under the ROC Curve (AUC).

% ============================================================
% V. RESULTS
% ============================================================
\section{Results}

\subsection{Compression Robustness}

Table~\ref{tab:compression} presents the AUC scores across compression levels.

\begin{table}[t]
\centering
\caption{AUC scores across compression levels on FF++ test set.}
\label{tab:compression}
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{c0} & \textbf{c23} & \textbf{c40} \\
\midrule
Spatial Only & \placeholder{AUC} & \placeholder{AUC} & \placeholder{AUC} \\
Frequency Only & \placeholder{AUC} & \placeholder{AUC} & \placeholder{AUC} \\
Hybrid (Ours) & \placeholder{AUC} & \placeholder{AUC} & \placeholder{AUC} \\
\bottomrule
\end{tabular}
\end{table}

The hybrid model demonstrates the smallest performance degradation from c0 to c40, with a drop of only \placeholder{X}\% compared to \placeholder{Y}\% for the spatial-only model.

\subsection{Ablation Study}

Table~\ref{tab:ablation} shows the contribution of each branch.

\begin{table}[t]
\centering
\caption{Ablation study: contribution of spatial and frequency branches (AUC on c40).}
\label{tab:ablation}
\begin{tabular}{lcc}
\toprule
\textbf{Configuration} & \textbf{AUC} & \textbf{F1} \\
\midrule
Spatial Only & \placeholder{AUC} & \placeholder{F1} \\
Frequency Only & \placeholder{AUC} & \placeholder{F1} \\
Hybrid (Ours) & \placeholder{AUC} & \placeholder{F1} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Cross-Dataset Generalization}

\begin{table}[t]
\centering
\caption{Cross-dataset evaluation: trained on FF++, tested on Celeb-DF v2.}
\label{tab:crossdataset}
\begin{tabular}{lccc}
\toprule
\textbf{Test Dataset} & \textbf{AUC} & \textbf{F1} & \textbf{Accuracy} \\
\midrule
FF++ (c23) & \placeholder{AUC} & \placeholder{F1} & \placeholder{Acc} \\
Celeb-DF v2 & \placeholder{AUC} & \placeholder{F1} & \placeholder{Acc} \\
\bottomrule
\end{tabular}
\end{table}

% ============================================================
% VI. DISCUSSION
% ============================================================
\section{Discussion}

Our results demonstrate that the frequency branch provides complementary information to the spatial branch, particularly under heavy compression. This is consistent with the observation that H.264 compression preferentially removes high-frequency spatial information, reducing the effectiveness of spatial-only detectors. The DWT subbands capture compression-level characteristics (e.g., the approximation subband retains more information under heavy compression, while detail subbands reveal different artifact patterns).

\textbf{Limitations.} Our study focuses on face-swap manipulations (Deepfakes, FaceSwap) and does not cover reenactment methods (Face2Face, NeuralTextures) due to compute constraints. The frequency branch uses single-level DWT; multi-scale analysis may capture additional information. No temporal modeling is employed.

% ============================================================
% VII. CONCLUSION
% ============================================================
\section{Conclusion}

We have presented a compression-aware hybrid framework for video deepfake detection that combines spatial features from EfficientNet-B0 with frequency-domain features from DWT-based analysis. Our systematic evaluation demonstrates that the hybrid approach consistently outperforms single-branch baselines across compression levels, with the most significant improvement observed under heavy compression (c40). The framework is reproducible, computationally efficient, and accompanied by an interactive demonstration system.

% ============================================================
% FUTURE WORK
% ============================================================
\section{Future Work}

Promising directions include multi-scale DWT decomposition, temporal frame sequence analysis, evaluation on additional manipulation types, simulation of social media re-compression pipelines, and integration of explainability techniques such as Grad-CAM.

% ============================================================
% REFERENCES
% ============================================================
\begin{thebibliography}{99}

\bibitem{rossler2019faceforensics}
A.~Rössler, D.~Cozzolino, L.~Verdoliva, C.~Riess, J.~Thies, and M.~Niessner,
``FaceForensics++: Learning to detect manipulated facial images,''
in \textit{Proc. IEEE/CVF Int. Conf. Comput. Vision (ICCV)}, 2019, pp.~1--11.

\bibitem{li2020celebdf}
Y.~Li, X.~Yang, P.~Sun, H.~Qi, and S.~Lyu,
``Celeb-DF: A large-scale challenging dataset for deepfake forensics,''
in \textit{Proc. IEEE/CVF Conf. Comput. Vision Pattern Recognit. (CVPR)}, 2020, pp.~3207--3216.

\bibitem{tan2019efficientnet}
M.~Tan and Q.~Le,
``EfficientNet: Rethinking model scaling for convolutional neural networks,''
in \textit{Proc. Int. Conf. Mach. Learn. (ICML)}, 2019, pp.~6105--6114.

\bibitem{durall2020watch}
R.~Durall, M.~Keuper, and J.~Keuper,
``Watch your up-convolution: CNN based generative deep neural networks are failing to reproduce spectral distributions,''
in \textit{Proc. IEEE/CVF Conf. Comput. Vision Pattern Recognit. (CVPR)}, 2020.

\bibitem{qian2020thinking}
Y.~Qian, G.~Yin, L.~Sheng, Z.~Chen, and J.~Shao,
``Thinking in frequency: Face forgery detection by mining frequency-aware clues,''
in \textit{Proc. Eur. Conf. Comput. Vision (ECCV)}, 2020.

\bibitem{frank2020leveraging}
J.~Frank, T.~Eisenhofer, L.~Schönherr, A.~Fischer, D.~Kolossa, and T.~Holz,
``Leveraging frequency analysis for deep fake image recognition,''
in \textit{Proc. Int. Conf. Mach. Learn. (ICML)}, 2020.

\bibitem{zhang2016mtcnn}
K.~Zhang, Z.~Zhang, Z.~Li, and Y.~Qiao,
``Joint face detection and alignment using multitask cascaded convolutional networks,''
\textit{IEEE Signal Process. Lett.}, vol.~23, no.~10, pp.~1499--1503, 2016.

\end{thebibliography}

\end{document}

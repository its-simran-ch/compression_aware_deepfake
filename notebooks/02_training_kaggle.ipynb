{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üèãÔ∏è Training & Evaluation on Kaggle\n",
                "## Compression-Aware Video Deepfake Detection\n",
                "\n",
                "**Pre-requisite:** Face crops uploaded as a Kaggle Dataset (`ffpp-faces-deepfake`)\n",
                "\n",
                "üìå **Settings ‚Üí Accelerator ‚Üí GPU T4 x2** (or P100)\n",
                "\n",
                "üìå **Add Data ‚Üí Your Datasets ‚Üí ffpp-faces-deepfake**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ‚ö†Ô∏è REPLACE with your actual GitHub repo URL\n",
                "GITHUB_REPO = 'https://github.com/YOUR_USERNAME/compression_aware_deepfake.git'\n",
                "\n",
                "!git clone {GITHUB_REPO} /kaggle/working/project\n",
                "%cd /kaggle/working/project\n",
                "!pip install -q -r requirements.txt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "print(f'PyTorch: {torch.__version__}')\n",
                "print(f'CUDA:    {torch.cuda.is_available()}')\n",
                "if torch.cuda.is_available():\n",
                "    print(f'GPU:     {torch.cuda.get_device_name(0)}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Verify Dataset\n",
                "\n",
                "Your uploaded dataset should be at:\n",
                "```\n",
                "/kaggle/input/ffpp-faces-deepfake/ffpp_faces/\n",
                "```\n",
                "\n",
                "‚ö†Ô∏è If your Kaggle dataset name is different, update `DATA_ROOT` below."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os, glob\n",
                "\n",
                "# ‚ö†Ô∏è UPDATE this path if your Kaggle dataset has a different name\n",
                "KAGGLE_INPUT = '/kaggle/input/ffpp-faces-deepfake'\n",
                "\n",
                "# The face crops might be directly in the dataset or inside a subfolder\n",
                "print('Kaggle input contents:')\n",
                "for item in os.listdir(KAGGLE_INPUT):\n",
                "    print(f'  {item}')\n",
                "\n",
                "# Find the metadata.csv\n",
                "csv_candidates = glob.glob(f'{KAGGLE_INPUT}/**/metadata.csv', recursive=True)\n",
                "if csv_candidates:\n",
                "    METADATA_CSV = csv_candidates[0]\n",
                "    DATA_ROOT = os.path.dirname(METADATA_CSV)\n",
                "    print(f'\\n‚úÖ Found metadata at: {METADATA_CSV}')\n",
                "    print(f'   Data root: {DATA_ROOT}')\n",
                "else:\n",
                "    print('\\n‚ùå metadata.csv not found! Check your Kaggle dataset.')\n",
                "    METADATA_CSV = None\n",
                "    DATA_ROOT = None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "if METADATA_CSV:\n",
                "    df = pd.read_csv(METADATA_CSV)\n",
                "    print(f'Total face crops: {len(df)}')\n",
                "    print(f'\\nBy split:       {dict(df[\"split\"].value_counts())}')\n",
                "    print(f'By label:       {dict(df[\"label\"].value_counts())}')\n",
                "    print(f'By compression: {dict(df[\"compression\"].value_counts())}')\n",
                "    \n",
                "    # Verify a sample image exists\n",
                "    sample_path = os.path.join(DATA_ROOT, df.iloc[0]['frame_path'])\n",
                "    print(f'\\nSample image exists: {os.path.exists(sample_path)}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Copy splits.json\n",
                "\n",
                "Since Kaggle input is read-only, we need to copy the splits file."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Copy splits if it exists in the dataset, otherwise regenerate\n",
                "os.makedirs('data/faceforensics', exist_ok=True)\n",
                "\n",
                "splits_candidates = glob.glob(f'{KAGGLE_INPUT}/**/splits.json', recursive=True)\n",
                "if splits_candidates:\n",
                "    !cp {splits_candidates[0]} data/faceforensics/splits.json\n",
                "    print('‚úÖ Copied existing splits.json')\n",
                "else:\n",
                "    # Generate from the metadata CSV\n",
                "    if METADATA_CSV:\n",
                "        unique_splits = df['split'].unique()\n",
                "        splits_dict = {}\n",
                "        for s in unique_splits:\n",
                "            # Get unique source video IDs per split\n",
                "            split_df = df[df['split'] == s]\n",
                "            vid_ids = sorted(split_df['video_id'].apply(lambda x: x.split('_')[0]).unique().tolist())\n",
                "            splits_dict[s] = vid_ids\n",
                "        \n",
                "        import json\n",
                "        with open('data/faceforensics/splits.json', 'w') as f:\n",
                "            json.dump(splits_dict, f, indent=2)\n",
                "        print('‚úÖ Generated splits.json from metadata')\n",
                "        for k, v in splits_dict.items():\n",
                "            print(f'  {k}: {len(v)} videos')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4Ô∏è‚É£ Train Hybrid Model (Main Experiment)\n",
                "\n",
                "Training on c23 + c40 with the hybrid (spatial + frequency) architecture.\n",
                "\n",
                "‚è±Ô∏è **~1‚Äì2 hours** on Kaggle T4 GPU"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Output directory (Kaggle writable area)\n",
                "!mkdir -p /kaggle/working/results/csv\n",
                "!mkdir -p /kaggle/working/results/checkpoints\n",
                "!mkdir -p /kaggle/working/results/plots\n",
                "\n",
                "!python src/training/train_ffpp.py \\\n",
                "    --metadata_csv {METADATA_CSV} \\\n",
                "    --data_root {DATA_ROOT} \\\n",
                "    --mode hybrid \\\n",
                "    --compressions c23 c40 \\\n",
                "    --epochs 15 \\\n",
                "    --batch_size 16 \\\n",
                "    --lr 1e-4 \\\n",
                "    --output_dir /kaggle/working/results \\\n",
                "    --experiment_name hybrid_c23_c40"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5Ô∏è‚É£ Train Baseline Models (Ablation)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Spatial-only baseline\n",
                "!python src/training/train_ffpp.py \\\n",
                "    --metadata_csv {METADATA_CSV} \\\n",
                "    --data_root {DATA_ROOT} \\\n",
                "    --mode spatial \\\n",
                "    --compressions c23 c40 \\\n",
                "    --epochs 15 \\\n",
                "    --batch_size 16 \\\n",
                "    --output_dir /kaggle/working/results \\\n",
                "    --experiment_name spatial_c23_c40"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Frequency-only baseline\n",
                "!python src/training/train_ffpp.py \\\n",
                "    --metadata_csv {METADATA_CSV} \\\n",
                "    --data_root {DATA_ROOT} \\\n",
                "    --mode frequency \\\n",
                "    --compressions c23 c40 \\\n",
                "    --epochs 15 \\\n",
                "    --batch_size 16 \\\n",
                "    --output_dir /kaggle/working/results \\\n",
                "    --experiment_name frequency_c23_c40"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6Ô∏è‚É£ Evaluate on Each Compression Level"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate hybrid model\n",
                "!python src/training/evaluate_compression_levels.py \\\n",
                "    --checkpoint /kaggle/working/results/checkpoints/best_hybrid_c23_c40.pth \\\n",
                "    --metadata_csv {METADATA_CSV} \\\n",
                "    --data_root {DATA_ROOT} \\\n",
                "    --mode hybrid \\\n",
                "    --compressions c0 c23 c40 \\\n",
                "    --output_csv /kaggle/working/results/csv/compression_eval_hybrid.csv"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate spatial model\n",
                "!python src/training/evaluate_compression_levels.py \\\n",
                "    --checkpoint /kaggle/working/results/checkpoints/best_spatial_c23_c40.pth \\\n",
                "    --metadata_csv {METADATA_CSV} \\\n",
                "    --data_root {DATA_ROOT} \\\n",
                "    --mode spatial \\\n",
                "    --compressions c0 c23 c40 \\\n",
                "    --output_csv /kaggle/working/results/csv/compression_eval_spatial.csv"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate frequency model\n",
                "!python src/training/evaluate_compression_levels.py \\\n",
                "    --checkpoint /kaggle/working/results/checkpoints/best_frequency_c23_c40.pth \\\n",
                "    --metadata_csv {METADATA_CSV} \\\n",
                "    --data_root {DATA_ROOT} \\\n",
                "    --mode frequency \\\n",
                "    --compressions c0 c23 c40 \\\n",
                "    --output_csv /kaggle/working/results/csv/compression_eval_frequency.csv"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7Ô∏è‚É£ Generate Plots"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Merge evaluation CSVs into ablation summary format\n",
                "import pandas as pd\n",
                "import os\n",
                "\n",
                "all_results = []\n",
                "for mode in ['hybrid', 'spatial', 'frequency']:\n",
                "    csv_path = f'/kaggle/working/results/csv/compression_eval_{mode}.csv'\n",
                "    if os.path.exists(csv_path):\n",
                "        df_eval = pd.read_csv(csv_path)\n",
                "        df_eval['mode'] = mode\n",
                "        df_eval['train_compressions'] = 'c23_c40'\n",
                "        df_eval['experiment'] = f'{mode}_c23_c40'\n",
                "        all_results.append(df_eval)\n",
                "\n",
                "if all_results:\n",
                "    df_all = pd.concat(all_results, ignore_index=True)\n",
                "    df_all.to_csv('/kaggle/working/results/csv/ablation_summary.csv', index=False)\n",
                "    print('‚úÖ Ablation summary created')\n",
                "    display(df_all)\n",
                "else:\n",
                "    print('No evaluation results found yet.')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate all paper-ready plots\n",
                "!python scripts/plot_results.py \\\n",
                "    --results_dir /kaggle/working/results/csv \\\n",
                "    --output_dir /kaggle/working/results/plots"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display the generated plots\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib.image as mpimg\n",
                "\n",
                "plot_dir = '/kaggle/working/results/plots'\n",
                "for fname in sorted(os.listdir(plot_dir)):\n",
                "    if fname.endswith('.png'):\n",
                "        print(f'\\n--- {fname} ---')\n",
                "        img = mpimg.imread(os.path.join(plot_dir, fname))\n",
                "        fig, ax = plt.subplots(figsize=(10, 6))\n",
                "        ax.imshow(img)\n",
                "        ax.axis('off')\n",
                "        plt.tight_layout()\n",
                "        plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8Ô∏è‚É£ Results Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Print final results table\n",
                "summary_csv = '/kaggle/working/results/csv/ablation_summary.csv'\n",
                "if os.path.exists(summary_csv):\n",
                "    df_summary = pd.read_csv(summary_csv)\n",
                "    \n",
                "    # Pivot table: AUC by mode √ó compression\n",
                "    pivot_auc = df_summary.pivot_table(values='auc', index='mode', columns='compression')\n",
                "    print('\\nüìä AUC by Model √ó Compression Level:')\n",
                "    print('='*50)\n",
                "    display(pivot_auc.round(4))\n",
                "    \n",
                "    # Pivot table: F1\n",
                "    pivot_f1 = df_summary.pivot_table(values='f1', index='mode', columns='compression')\n",
                "    print('\\nüìä F1 by Model √ó Compression Level:')\n",
                "    print('='*50)\n",
                "    display(pivot_f1.round(4))\n",
                "    \n",
                "    # LaTeX for paper\n",
                "    print('\\nüìù LaTeX table (AUC) for paper:')\n",
                "    print(pivot_auc.round(4).to_latex())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9Ô∏è‚É£ Save Outputs\n",
                "\n",
                "Kaggle saves everything in `/kaggle/working/` as output. You can:\n",
                "1. **Download** the `results/` folder from the notebook output\n",
                "2. **Download** the model checkpoints from `results/checkpoints/`\n",
                "3. Use these in your Streamlit demo and paper"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# List all output files\n",
                "print('üì¶ Output files for download:')\n",
                "for root, dirs, files in os.walk('/kaggle/working/results'):\n",
                "    for f in files:\n",
                "        full = os.path.join(root, f)\n",
                "        size = os.path.getsize(full) / (1024*1024)  # MB\n",
                "        rel = os.path.relpath(full, '/kaggle/working')\n",
                "        print(f'  {rel}  ({size:.1f} MB)')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        },
        "accelerator": "GPU"
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèãÔ∏è Training on Google Colab\n",
    "## Compression-Aware Video Deepfake Detection\n",
    "\n",
    "This notebook trains all 3 model variants directly using face crops on Google Drive.\n",
    "\n",
    "**Pre-requisite:** Face crops already extracted to `/content/drive/MyDrive/ffpp_faces/`\n",
    "\n",
    "üìå **Runtime ‚Üí Change runtime type ‚Üí GPU (T4)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo and install dependencies\n",
    "!git clone https://github.com/its-simran-ch/compression_aware_deepfake.git /content/project\n",
    "%cd /content/project\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU + packages\n",
    "import torch\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA:    {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU:     {torch.cuda.get_device_name(0)}')\n",
    "\n",
    "from facenet_pytorch import MTCNN\n",
    "print('MTCNN:   OK')\n",
    "import pywt\n",
    "print('PyWavelets: OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Verify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "DATA_ROOT = '/content/drive/MyDrive/ffpp_faces'\n",
    "METADATA_CSV = f'{DATA_ROOT}/metadata.csv'\n",
    "OUTPUT_DIR = '/content/drive/MyDrive/deepfake_results'  # Save results to Drive!\n",
    "\n",
    "df = pd.read_csv(METADATA_CSV)\n",
    "print(f'‚úÖ Total face crops: {len(df)}')\n",
    "print(f'\\nBy split:')\n",
    "print(df['split'].value_counts())\n",
    "print(f'\\nBy label:')\n",
    "print(df['label'].value_counts())\n",
    "print(f'\\nBy compression:')\n",
    "print(df['compression'].value_counts())\n",
    "\n",
    "# Verify a sample image\n",
    "sample = df.iloc[0]\n",
    "sample_path = os.path.join(DATA_ROOT, sample['frame_path'])\n",
    "print(f'\\nSample image exists: {os.path.exists(sample_path)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Quick Sanity Check (2-3 min)\n",
    "\n",
    "Run a tiny test to make sure training works before the full run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test: 1 epoch, limited samples\n",
    "!python src/training/train_ffpp.py \\\n",
    "    --metadata_csv {METADATA_CSV} \\\n",
    "    --data_root {DATA_ROOT} \\\n",
    "    --mode hybrid \\\n",
    "    --compressions c23 \\\n",
    "    --epochs 1 \\\n",
    "    --batch_size 8 \\\n",
    "    --max_train_samples 100 \\\n",
    "    --max_val_samples 50 \\\n",
    "    --output_dir /content/test_run \\\n",
    "    --experiment_name quick_test\n",
    "\n",
    "print('\\n‚úÖ Sanity check passed! Training pipeline works.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4Ô∏è‚É£ Train Hybrid Model (Main Experiment)\n",
    "\n",
    "‚è±Ô∏è **~1.5-2 hours** on T4 GPU\n",
    "\n",
    "Results are saved to Google Drive so they persist even if Colab disconnects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/training/train_ffpp.py \\\n",
    "    --metadata_csv {METADATA_CSV} \\\n",
    "    --data_root {DATA_ROOT} \\\n",
    "    --mode hybrid \\\n",
    "    --compressions c23 c40 \\\n",
    "    --epochs 15 \\\n",
    "    --batch_size 16 \\\n",
    "    --lr 1e-4 \\\n",
    "    --output_dir {OUTPUT_DIR} \\\n",
    "    --experiment_name hybrid_c23_c40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Train Baseline Models (Ablation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial-only baseline\n",
    "!python src/training/train_ffpp.py \\\n",
    "    --metadata_csv {METADATA_CSV} \\\n",
    "    --data_root {DATA_ROOT} \\\n",
    "    --mode spatial \\\n",
    "    --compressions c23 c40 \\\n",
    "    --epochs 15 \\\n",
    "    --batch_size 16 \\\n",
    "    --lr 1e-4 \\\n",
    "    --output_dir {OUTPUT_DIR} \\\n",
    "    --experiment_name spatial_c23_c40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency-only baseline\n",
    "!python src/training/train_ffpp.py \\\n",
    "    --metadata_csv {METADATA_CSV} \\\n",
    "    --data_root {DATA_ROOT} \\\n",
    "    --mode frequency \\\n",
    "    --compressions c23 c40 \\\n",
    "    --epochs 15 \\\n",
    "    --batch_size 16 \\\n",
    "    --lr 1e-4 \\\n",
    "    --output_dir {OUTPUT_DIR} \\\n",
    "    --experiment_name frequency_c23_c40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Evaluate Per Compression Level\n",
    "\n",
    "This evaluates each model on c23 and c40 separately ‚Äî crucial for the paper's compression robustness analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/content/project')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from torch.utils.data import DataLoader\n",
    "from src.datasets.ffpp_dataset import FFPPFrameDataset\n",
    "from src.models.fusion_classifier import HybridDeepfakeClassifier\n",
    "from src.utils.metrics import compute_metrics\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "eval_results = []\n",
    "\n",
    "for mode in ['hybrid', 'spatial', 'frequency']:\n",
    "    ckpt_path = f'{OUTPUT_DIR}/checkpoints/best_{mode}_c23_c40.pth'\n",
    "    if not os.path.exists(ckpt_path):\n",
    "        print(f'‚ö†Ô∏è Skipping {mode} ‚Äî checkpoint not found')\n",
    "        continue\n",
    "    \n",
    "    # Load model\n",
    "    model = HybridDeepfakeClassifier(mode=mode, pretrained_spatial=False).to(device)\n",
    "    ckpt = torch.load(ckpt_path, map_location=device, weights_only=False)\n",
    "    model.load_state_dict(ckpt['model_state_dict'])\n",
    "    model.eval()\n",
    "    print(f'\\n{\"=\"*50}')\n",
    "    print(f'Evaluating: {mode} (best epoch {ckpt[\"epoch\"]}, train AUC={ckpt[\"val_auc\"]:.4f})')\n",
    "    \n",
    "    for comp in ['c23', 'c40']:\n",
    "        include_dwt = mode in ('frequency', 'hybrid')\n",
    "        test_ds = FFPPFrameDataset(\n",
    "            metadata_csv=METADATA_CSV,\n",
    "            root_dir=DATA_ROOT,\n",
    "            split='test',\n",
    "            compressions=[comp],\n",
    "            include_dwt=include_dwt,\n",
    "        )\n",
    "        test_loader = DataLoader(test_ds, batch_size=32, shuffle=False, num_workers=2)\n",
    "        \n",
    "        all_labels, all_probs = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                labels = batch['label'].float()\n",
    "                rgb = batch['rgb'].to(device) if mode != 'frequency' else None\n",
    "                dwt = batch['dwt'].to(device) if mode != 'spatial' else None\n",
    "                logits = model(rgb_input=rgb, dwt_input=dwt)\n",
    "                probs = torch.sigmoid(logits).cpu().numpy()\n",
    "                all_probs.extend(probs)\n",
    "                all_labels.extend(labels.numpy())\n",
    "        \n",
    "        preds = (np.array(all_probs) >= 0.5).astype(int)\n",
    "        metrics = compute_metrics(np.array(all_labels), preds, np.array(all_probs))\n",
    "        \n",
    "        eval_results.append({\n",
    "            'mode': mode, 'compression': comp,\n",
    "            'accuracy': metrics['accuracy'], 'f1': metrics['f1'],\n",
    "            'auc': metrics['auc'],\n",
    "        })\n",
    "        print(f'  {comp}: Acc={metrics[\"accuracy\"]:.4f}  F1={metrics[\"f1\"]:.4f}  AUC={metrics[\"auc\"]:.4f}')\n",
    "\n",
    "# Save results\n",
    "df_eval = pd.DataFrame(eval_results)\n",
    "df_eval.to_csv(f'{OUTPUT_DIR}/csv/compression_eval_summary.csv', index=False)\n",
    "print(f'\\n\\nüìä Results Summary:')\n",
    "display(df_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Generate Paper Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "os.makedirs(f'{OUTPUT_DIR}/plots', exist_ok=True)\n",
    "\n",
    "# ‚îÄ‚îÄ Compression Robustness Line Plot ‚îÄ‚îÄ\n",
    "if len(eval_results) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    colors = {'hybrid': '#2ec4b6', 'spatial': '#4361ee', 'frequency': '#f77f00'}\n",
    "    \n",
    "    for mode in ['hybrid', 'spatial', 'frequency']:\n",
    "        mode_data = [r for r in eval_results if r['mode'] == mode]\n",
    "        if mode_data:\n",
    "            comps = [r['compression'] for r in mode_data]\n",
    "            aucs = [r['auc'] for r in mode_data]\n",
    "            ax.plot(comps, aucs, 'o-', label=mode.capitalize(),\n",
    "                    color=colors[mode], linewidth=2.5, markersize=10)\n",
    "    \n",
    "    ax.set_xlabel('Compression Level', fontsize=12)\n",
    "    ax.set_ylabel('AUC', fontsize=12)\n",
    "    ax.set_title('Detection Performance Across Compression Levels', fontsize=14)\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(0.5, 1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{OUTPUT_DIR}/plots/compression_robustness.png', dpi=200, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print('Saved: compression_robustness.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ Ablation Bar Chart ‚îÄ‚îÄ\n",
    "if len(eval_results) > 0:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    colors = {'hybrid': '#2ec4b6', 'spatial': '#4361ee', 'frequency': '#f77f00'}\n",
    "    \n",
    "    # Average AUC across compressions\n",
    "    avg_metrics = {}\n",
    "    for mode in ['hybrid', 'spatial', 'frequency']:\n",
    "        mode_data = [r for r in eval_results if r['mode'] == mode]\n",
    "        if mode_data:\n",
    "            avg_metrics[mode] = {\n",
    "                'auc': np.mean([r['auc'] for r in mode_data]),\n",
    "                'f1': np.mean([r['f1'] for r in mode_data]),\n",
    "            }\n",
    "    \n",
    "    modes = list(avg_metrics.keys())\n",
    "    aucs = [avg_metrics[m]['auc'] for m in modes]\n",
    "    f1s = [avg_metrics[m]['f1'] for m in modes]\n",
    "    bar_colors = [colors[m] for m in modes]\n",
    "    \n",
    "    bars1 = ax1.bar(modes, aucs, color=bar_colors, alpha=0.85, width=0.5)\n",
    "    for bar, val in zip(bars1, aucs):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "                 f'{val:.4f}', ha='center', fontweight='bold')\n",
    "    ax1.set_ylabel('AUC'); ax1.set_title('Average AUC by Model')\n",
    "    ax1.set_ylim(0.5, 1.05); ax1.grid(True, alpha=0.2, axis='y')\n",
    "    \n",
    "    bars2 = ax2.bar(modes, f1s, color=bar_colors, alpha=0.85, width=0.5)\n",
    "    for bar, val in zip(bars2, f1s):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "                 f'{val:.4f}', ha='center', fontweight='bold')\n",
    "    ax2.set_ylabel('F1 Score'); ax2.set_title('Average F1 by Model')\n",
    "    ax2.set_ylim(0.5, 1.05); ax2.grid(True, alpha=0.2, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{OUTPUT_DIR}/plots/ablation_comparison.png', dpi=200, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print('Saved: ablation_comparison.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ Training Curves ‚îÄ‚îÄ\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "colors = {'hybrid': '#2ec4b6', 'spatial': '#4361ee', 'frequency': '#f77f00'}\n",
    "\n",
    "for mode in ['hybrid', 'spatial', 'frequency']:\n",
    "    log_path = f'{OUTPUT_DIR}/csv/train_log_{mode}_c23_c40.csv'\n",
    "    if not os.path.exists(log_path):\n",
    "        continue\n",
    "    df_log = pd.read_csv(log_path)\n",
    "    color = colors[mode]\n",
    "    \n",
    "    axes[0].plot(df_log['epoch'], df_log['val_loss'], label=mode.capitalize(),\n",
    "                 color=color, linewidth=2)\n",
    "    axes[1].plot(df_log['epoch'], df_log['val_auc'], label=mode.capitalize(),\n",
    "                 color=color, linewidth=2, marker='o', markersize=4)\n",
    "\n",
    "axes[0].set_xlabel('Epoch'); axes[0].set_ylabel('Validation Loss')\n",
    "axes[0].set_title('Validation Loss'); axes[0].legend(); axes[0].grid(True, alpha=0.3)\n",
    "axes[1].set_xlabel('Epoch'); axes[1].set_ylabel('Validation AUC')\n",
    "axes[1].set_title('Validation AUC'); axes[1].legend(); axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/plots/training_curves.png', dpi=200, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved: training_curves.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Results for Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final results table\n",
    "if len(eval_results) > 0:\n",
    "    df_final = pd.DataFrame(eval_results)\n",
    "    \n",
    "    # Pivot: rows=mode, cols=compression, values=AUC\n",
    "    pivot_auc = df_final.pivot_table(values='auc', index='mode', columns='compression')\n",
    "    print('üìä AUC by Model √ó Compression:')\n",
    "    print('='*40)\n",
    "    display(pivot_auc.round(4))\n",
    "    \n",
    "    pivot_f1 = df_final.pivot_table(values='f1', index='mode', columns='compression')\n",
    "    print('\\nüìä F1 by Model √ó Compression:')\n",
    "    print('='*40)\n",
    "    display(pivot_f1.round(4))\n",
    "    \n",
    "    # LaTeX for paper\n",
    "    print('\\nüìù LaTeX table (paste into paper):')\n",
    "    print(pivot_auc.round(4).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all saved files\n",
    "print('üì¶ All results saved to Google Drive:')\n",
    "for root, dirs, files in os.walk(OUTPUT_DIR):\n",
    "    for f in files:\n",
    "        full = os.path.join(root, f)\n",
    "        size = os.path.getsize(full) / (1024*1024)\n",
    "        rel = os.path.relpath(full, OUTPUT_DIR)\n",
    "        print(f'  {rel}  ({size:.1f} MB)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
